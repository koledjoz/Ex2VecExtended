{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aac8ff8-c61c-4650-859e-7b17918c7ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torcheval.metrics import ReciprocalRank\n",
    "\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd2317d3-7d43-4c15-bde2-72c183f9275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_excluding(n, x, a):\n",
    "    if x == -1:\n",
    "        return [num for num in range(1,n+1) if num != a ]\n",
    "    \n",
    "    if x > n - 1:\n",
    "        raise ValueError(\"Cannot sample more elements than available excluding 'a'\")\n",
    "    \n",
    "    # Sample x numbers from 1 to n-1\n",
    "    sampled = random.sample(range(1, n), x)\n",
    "\n",
    "    # Map values >= a to skip 'a'\n",
    "    return [num if num < a else num + 1 for num in sampled]\n",
    "# ok, lets look at what we need right here and there\n",
    "\n",
    "class Ex2VecDataset(Dataset):\n",
    "    def __init__(self, data_path, usage_dict_path, timedeltas_list_path, history_size=3500, sample_negative=999, max_padding=256):\n",
    "        self.data_path = data_path\n",
    "        self.usage_dict_path = usage_dict_path\n",
    "        self.history_size = history_size\n",
    "        self.sample_negative = sample_negative\n",
    "\n",
    "        self.data = pd.read_parquet(self.data_path)\n",
    "\n",
    "        self.max_user = self.data['user_id'].max()\n",
    "        self.max_item = self.data['track_id'].max()\n",
    "\n",
    "        self.data.set_index(['user_id', 'track_id'], inplace=True, drop=False)\n",
    "\n",
    "        self.max_padding=max_padding\n",
    "\n",
    "        with open(usage_dict_path) as file:\n",
    "            self.use_dict = {int(key) : set(value) for key, value in json.load(file).items()}\n",
    "\n",
    "        with h5py.File(timedeltas_list_path, 'r') as f:\n",
    "            self.offsets = f['offsets'][:]\n",
    "            self.timestamps_flat = f['timestamps_flat'][:]\n",
    "\n",
    "            self.pos_dict = {tuple(x) : i for i,x in enumerate(tqdm(f['user_item']))}\n",
    "    \n",
    "            total_size = (self.max_user + 1) * (self.max_item + 1)\n",
    "            self.pos_array = np.full(total_size, -1, dtype=np.int32)\n",
    "            for i, (user, item) in enumerate(tqdm(f['user_item'])):\n",
    "                flat_index = user * (self.max_item + 1) + item\n",
    "                self.pos_array[flat_index] = i\n",
    "\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # @profile\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        user = self.data.iloc[idx]['user_id']\n",
    "        pred_item = self.data.iloc[idx]['track_id']\n",
    "        ts = self.data.iloc[idx]['ts']\n",
    "\n",
    "        if pred_item not in self.use_dict[user]:\n",
    "            return None\n",
    "\n",
    "        if self.sample_negative != -1:\n",
    "            true_vals = np.zeros(self.sample_negative + 1, dtype=np.float32)\n",
    "            samples = np.empty(self.sample_negative + 1, dtype=np.int32)\n",
    "            timedeltas = np.zeros((self.sample_negative + 1, self.max_padding), dtype=np.float32)\n",
    "        else:\n",
    "            true_vals = np.zeros(self.max_item)\n",
    "            samples = np.empty(self.max_item, dtype=np.int32)\n",
    "            timedeltas = np.zeros((self.max_item, self.max_padding), dtype=np.float32)\n",
    "        true_vals[-1] = 1.0\n",
    "    \n",
    "        \n",
    "        samples[:-1] = sample_excluding(self.max_item, self.sample_negative, pred_item)\n",
    "        samples[-1] = pred_item\n",
    "    \n",
    "        # Vectorized flat index computation\n",
    "        flat_indices = user * (self.max_item + 1) + samples\n",
    "        idx_items = self.pos_array[flat_indices]\n",
    "    \n",
    "        \n",
    "        valid_mask = idx_items != -1\n",
    "        valid_indices = np.nonzero(valid_mask)[0]\n",
    "        valid_pos = idx_items[valid_mask]\n",
    "    \n",
    "        starts = self.offsets[valid_pos, 0]\n",
    "        lengths = self.offsets[valid_pos, 1]\n",
    "        ends = starts + lengths\n",
    "    \n",
    "        for i, (start, end, length, sample_idx) in enumerate(zip(starts, ends, lengths, valid_indices)):\n",
    "            timedeltas[sample_idx, :length] = ts - self.timestamps_flat[start:end]\n",
    "\n",
    "        weights = timedeltas > 0\n",
    "        \n",
    "        return {\n",
    "            'user_id' : torch.tensor(user),\n",
    "            'predict_items' : torch.tensor(samples),\n",
    "            'real_values' : torch.tensor(true_vals),\n",
    "            'timedeltas' : torch.from_numpy(timedeltas),\n",
    "            'weights' : torch.from_numpy(weights.astype(np.float32))\n",
    "        }\n",
    "        \n",
    "# @profile\n",
    "def collate_fn(batch):\n",
    "    # Remove None entries\n",
    "    batch = [x for x in batch if x is not None]\n",
    "    \n",
    "    if not batch:\n",
    "        return None  # Signal to skip this batch\n",
    "    \n",
    "    # Stack each field in the batch\n",
    "    collated_batch = {}\n",
    "    keys = batch[0].keys()\n",
    "    for key in keys:\n",
    "        collated_batch[key] = torch.stack([sample[key] for sample in batch])\n",
    "\n",
    "    return collated_batch\n",
    "\n",
    "class Ex2VecOriginal(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Ex2VecOriginal, self).__init__()\n",
    "        self.config = config\n",
    "        self.n_users = config['n_users']\n",
    "        self.n_items = config['n_items']\n",
    "        self.latend_d = config['latent_d']\n",
    "\n",
    "        self.global_lamb = torch.nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "        self.user_lamb = torch.nn.Embedding(self.n_users+1, 1)\n",
    "\n",
    "        self.user_bias = torch.nn.Embedding(self.n_users+1, 1)\n",
    "        self.item_bias = torch.nn.Embedding(self.n_items+1, 1)\n",
    "\n",
    "        self.alpha = torch.nn.Parameter(torch.tensor(1.0))\n",
    "        self.beta = torch.nn.Parameter(torch.tensor(-0.065))\n",
    "        self.gamma = torch.nn.Parameter(torch.tensor(0.5))\n",
    "\n",
    "        self.cutoff = torch.nn.Parameter(torch.tensor(3.0))\n",
    "\n",
    "        self.embedding_user = torch.nn.Embedding(\n",
    "            num_embeddings=self.n_users+1, embedding_dim=self.latend_d\n",
    "        )\n",
    "\n",
    "        self.embedding_item = torch.nn.Embedding(\n",
    "            num_embeddings=self.n_items+1, embedding_dim=self.latend_d\n",
    "        )\n",
    "\n",
    "        self.logistic = torch.nn.Sigmoid()\n",
    "\n",
    "    \n",
    "    def forward(self, user_id, item_id, timedeltas, weights):\n",
    "        user_emb = self.embedding_user(user_id).unsqueeze(1)\n",
    "        item_emb = self.embedding_item(item_id)\n",
    "\n",
    "        u_bias = self.user_bias(user_id)\n",
    "        i_bias = self.item_bias(item_id).squeeze(-1)\n",
    "\n",
    "        # base_dist = torch.sqrt((user_emb - item_emb)**2).sum(dim=2)\n",
    "        base_dist = torch.norm(user_emb - item_emb, dim=-1)\n",
    "\n",
    "        lamb = self.global_lamb + self.user_lamb(user_id).unsqueeze(-1)\n",
    "\n",
    "\n",
    "        # print('ORIG TIMEDELTAS', timedeltas)\n",
    "\n",
    "        # print('PRE POW DELTAS', (timedeltas + self.cutoff) * weights)\n",
    "\n",
    "        timedeltas = torch.pow(torch.clamp(timedeltas + self.cutoff, min=1e-6), -0.5)\n",
    "        # print('TIMEDELTAS 1', timedeltas)\n",
    "        # if torch.isnan(timedeltas).any():\n",
    "        #     print(f'crashing {15324/0}')\n",
    "\n",
    "        timedeltas = timedeltas * weights\n",
    "        # print('TIMEDELTAS 2', timedeltas)\n",
    "\n",
    "        # if torch.isnan(timedeltas).any():\n",
    "        #     print(f'crashing {15324/0}')\n",
    "\n",
    "        timedeltas = timedeltas * weights\n",
    "\n",
    "        base_level = lamb * timedeltas\n",
    "\n",
    "        # print('LAMB', lamb)\n",
    "        # print('TIMEDELTAS', timedeltas)\n",
    "\n",
    "        # if torch.isnan(base_level).any():\n",
    "        #     print('NONE FOUND BASE LEVEL 1 ', base_level)\n",
    "        #     print(f'crashing {15324/0}')\n",
    "\n",
    "        # print('PRE SUM', base_level)\n",
    "\n",
    "        base_level = torch.sum(base_level, axis=2)\n",
    "\n",
    "        # if torch.isnan(base_level).any:\n",
    "        #     print('NONE FOUND BASE LEVEL 2 ', base_level)\n",
    "        #     print(f'crashing {15324/0}')\n",
    "\n",
    "        output = torch.maximum(torch.zeros_like(base_dist), base_dist - base_level)\n",
    "\n",
    "        # if torch.isnan(output).any():\n",
    "        #     print('NONE FOUND OUTPUT', output)\n",
    "        #     print(f'crashing {15324/0}')\n",
    "\n",
    "        I = self.alpha * output  + self.beta * torch.pow(output, 2) + self.gamma + u_bias + i_bias\n",
    "      \n",
    "        return I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74be171d-97fa-412b-91db-3c0e14c0e7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 4892757/4892757 [00:30<00:00, 159774.12it/s]\n",
      "100%|███████████████████████████████████████████████████| 4892757/4892757 [00:30<00:00, 162081.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# need to generate all the results i need \n",
    "dataset_test = Ex2VecDataset('sorted_data.parquet', 'test_dict.json', 'interactions.h5', sample_negative=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a72849fd-30ff-40d4-9056-eb192fb14fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_test = DataLoader(dataset_test, batch_size=1024, num_workers = 8, shuffle=False, collate_fn=collate_fn, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56c6cdfd-b11b-4895-869a-72eec90b8c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "config = {\n",
    "    'n_users' : dataset_test.max_user,\n",
    "    'n_items' : dataset_test.max_item,\n",
    "    'latent_d' : 64\n",
    "}\n",
    "\n",
    "model = Ex2VecOriginal(config).to(device)\n",
    "\n",
    "checkpoint = torch.load('orig_model_epoch_29.pt', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee163fe2-2d37-40aa-8ede-da61f540dc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|█████████████████████████▍                                  | 6910/16269 [34:22<37:27,  4.16it/s]"
     ]
    }
   ],
   "source": [
    "# df_all = pd.DataFrame()\n",
    "\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "\n",
    "pbar_test = tqdm(enumerate(loader_test), total=len(loader_test))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in pbar_test:\n",
    "        \n",
    "        \n",
    "        if batch is None:\n",
    "            pbar_test.update(1)\n",
    "            continue\n",
    "            \n",
    "        real = batch['real_values'].to(device)\n",
    "        user_id = batch['user_id'].to(device)\n",
    "        predict_items = batch['predict_items'].to(device)\n",
    "        timedeltas = batch['timedeltas'].to(device)\n",
    "        weights = batch['weights'].to(device)\n",
    "\n",
    "        output = model(user_id, predict_items, timedeltas, weights)\n",
    "\n",
    "        # Convert to numpy\n",
    "        scores = output.cpu().numpy()\n",
    "        item_indices = predict_items.cpu().numpy()\n",
    "\n",
    "        batch_size, item_count = scores.shape\n",
    "        max_item_index = item_indices.max()\n",
    "\n",
    "        result = np.full((batch_size, max_item_index + 1), np.nan)  # NaN for missing values\n",
    "\n",
    "        rows = np.arange(batch_size).reshape(-1, 1)\n",
    "        result[rows, item_indices] = scores  # Assign each score to the right item index column\n",
    "\n",
    "        df_batch = pd.DataFrame(result)\n",
    "        df_batch[0] = predict_items[:, -1].cpu().detach().numpy()\n",
    "\n",
    "        # # Optionally add user ID to keep track of rows\n",
    "        # df_batch[\"user_id\"] = user_id.cpu().numpy()\n",
    "        # df_batch.set_index(\"user_id\", inplace=True)\n",
    "\n",
    "        all_predictions.append(df_batch)\n",
    "\n",
    "        # df_all = pd.concat([df_all, df_batch], axis=0, ignore_index=True, copy=False)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            df_all = pd.concat(all_predictions, axis=0, ignore_index=True, copy=False)\n",
    "            df_all.to_parquet(f'./original_results/original_results_ordered_{i//1000}.parquet')\n",
    "            all_predictions = []\n",
    "\n",
    "        pbar_test.update(1)\n",
    "\n",
    "df_all = pd.concat(all_predictions, axis=0, ignore_index=True, copy=False)\n",
    "df_all.to_parquet(f'./original_results/original_results_ordered_17last.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
