{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0b722f4-8369-4a7a-81f1-044b06e95688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from math import gcd\n",
    "import time\n",
    "\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, get_worker_info\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torcheval.metrics import ReciprocalRank\n",
    "\n",
    "from sklearn.metrics import ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12f2b048-26ca-4f27-b9ef-eb587e9f044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ex2Vec(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Ex2Vec, self).__init__()\n",
    "        self.config = config\n",
    "        self.n_users = config['n_users']\n",
    "        self.n_items = config['n_items']\n",
    "        self.latend_d = config['latent_d']\n",
    "\n",
    "        self.global_lamb = torch.nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "        self.user_lamb = torch.nn.Embedding(self.n_users+1, 1)\n",
    "\n",
    "        self.user_bias = torch.nn.Embedding(self.n_users+1, 1)\n",
    "        self.item_bias = torch.nn.Embedding(self.n_items+1, 1)\n",
    "\n",
    "        self.alpha = torch.nn.Parameter(torch.tensor(1.0))\n",
    "        self.beta = torch.nn.Parameter(torch.tensor(-0.065))\n",
    "        self.gamma = torch.nn.Parameter(torch.tensor(0.5))\n",
    "\n",
    "        self.cutoff = torch.nn.Parameter(torch.tensor(3.0))\n",
    "\n",
    "        self.embedding_user = torch.nn.Embedding(\n",
    "            num_embeddings=self.n_users+1, embedding_dim=self.latend_d\n",
    "        )\n",
    "\n",
    "        self.embedding_item = torch.nn.Embedding(\n",
    "            num_embeddings=self.n_items+1, embedding_dim=self.latend_d\n",
    "        )\n",
    "\n",
    "        self.logistic = torch.nn.Sigmoid()\n",
    "\n",
    "        self.smooth = torch.nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "        self.force = torch.nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "\n",
    "    def forward(self, user_index, pred_item_indices, history_item_indices, history_timedeltas, history_weights):\n",
    "        user_emb = self.embedding_user(user_index).unsqueeze(1)\n",
    "\n",
    "        pred_items_emb = self.embedding_item(pred_item_indices)\n",
    "\n",
    "        dist_user_item = torch.norm(user_emb - pred_items_emb, dim=2)\n",
    "\n",
    "        history_items_emb = self.embedding_item(history_item_indices)\n",
    "\n",
    "        pred_items_emb = pred_items_emb.unsqueeze(1)\n",
    "        history_items_emb = history_items_emb.unsqueeze(2)\n",
    "\n",
    "        dist = torch.norm(pred_items_emb - history_items_emb, dim=-1)\n",
    "\n",
    "        dist = self.logistic(self.smooth / (1 + dist) - self.force * self.smooth) / self.logistic(self.smooth - self.force * self.smooth)\n",
    "\n",
    "        history_timedeltas = (history_timedeltas + self.cutoff) ** -0.5\n",
    "\n",
    "        history_timedeltas = history_timedeltas * history_weights\n",
    "\n",
    "        result = history_timedeltas.unsqueeze(2) * dist\n",
    "\n",
    "        lamb = self.global_lamb + self.user_lamb(user_index)\n",
    "\n",
    "        result = lamb.unsqueeze(2) * result\n",
    "        \n",
    "        result = torch.sum(result, axis=1)\n",
    "        \n",
    "        output = torch.maximum(torch.zeros_like(dist_user_item), dist_user_item - result)\n",
    "\n",
    "        u_bias = self.user_bias(user_index)\n",
    "        i_bias = self.item_bias(pred_item_indices).squeeze(-1)\n",
    "\n",
    "        I = self.alpha * output  + self.beta * torch.pow(output, 2) + self.gamma + u_bias + i_bias\n",
    "      \n",
    "        return I\n",
    "\n",
    "\n",
    "\n",
    "def sample_excluding(n, x, a):\n",
    "    if x == -1:\n",
    "        return [num for num in range(1,n+1) if num != a ]\n",
    "    \n",
    "    if x > n - 1:\n",
    "        raise ValueError(\"Cannot sample more elements than available excluding 'a'\")\n",
    "    \n",
    "    # Sample x numbers from 1 to n-1\n",
    "    sampled = random.sample(range(1, n), x)\n",
    "\n",
    "    # Map values >= a to skip 'a'\n",
    "    return [num if num < a else num + 1 for num in sampled]\n",
    "# ok, lets look at what we need right here and there\n",
    "\n",
    "\n",
    "class DeezerDataset(Dataset):\n",
    "    def __init__(self, data_path, usage_dict_path, grouping_size=1000, sample_negative=999, history_size=128, seed=None):\n",
    "        print('Starting dataset initialization')\n",
    "\n",
    "        self.grouping_size = grouping_size\n",
    "\n",
    "        with open(usage_dict_path) as file:\n",
    "            self.use_dict = {int(key) : set(value) for key, value in json.load(file).items()}\n",
    "        \n",
    "        self.data = pd.read_parquet(data_path)\n",
    "\n",
    "        self.sample_negative = sample_negative\n",
    "        self.history_size=128\n",
    "\n",
    "        self.seed = seed\n",
    "\n",
    "        self.max_user = self.data['user_id'].max()\n",
    "        self.max_item = self.data['track_id'].max()\n",
    "\n",
    "        print('Dataset initialized')\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # t0 = time.time()\n",
    "\n",
    "        pred_user_id = self.data.iloc[idx]['user_id']\n",
    "        pred_item = self.data.iloc[idx]['track_id']\n",
    "\n",
    "        if pred_item not in self.use_dict[pred_user_id]:\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        pred_items = np.append(np.array(sample_excluding(self.max_item, self.sample_negative, pred_item)), pred_item)\n",
    "        true_vals = np.append(np.array([0.0 for _ in range(len(pred_items)-1)]), 1.0)\n",
    "\n",
    "        history = self.data.iloc[max(idx-self.history_size, 0):idx]\n",
    "        history = history[history['user_id'] == pred_user_id]\n",
    "\n",
    "        ts = self.data.iloc[idx]['ts']\n",
    "        timedeltas = (ts - history['ts']).to_numpy()\n",
    "\n",
    "        history_items = history['track_id'].to_numpy()\n",
    "        weights = np.ones_like(history_items)\n",
    "\n",
    "        # timedeltas = np.pad(timedeltas, (0, self.history_size-len(timedeltas)))\n",
    "        # history_items = np.pad(history_items, (0, self.history_size-len(history_items)))\n",
    "        # weights = np.pad(weights, (0, self.history_size-len(weights)))\n",
    "\n",
    "        timedeltas = np.pad(timedeltas, (0, self.history_size - len(timedeltas)), mode='constant', constant_values=0)\n",
    "        history_items = np.pad(history_items, (0, self.history_size - len(history_items)), mode='constant', constant_values=0)\n",
    "        weights = np.pad(weights, (0, self.history_size - len(weights)), mode='constant', constant_values=0)\n",
    "\n",
    "        \n",
    "        return {\n",
    "            'user_id' : torch.tensor(pred_user_id),\n",
    "            'predict_items' : torch.tensor(pred_items),\n",
    "            'real_values' : torch.tensor(true_vals),\n",
    "            'history_items' : torch.tensor(history_items),\n",
    "            'timedeltas' : torch.tensor(timedeltas),\n",
    "            'weights' : torch.tensor(weights)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d11406b-d1a8-472e-9d4c-e301e89db9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataset initialization\n",
      "Dataset initialized\n"
     ]
    }
   ],
   "source": [
    "dataset_test = DeezerDataset('sorted_data.parquet', 'test_dict.json', sample_negative=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4553f736-7652-4834-b094-90cc45748042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Remove None entries\n",
    "    batch = [x for x in batch if x is not None]\n",
    "    \n",
    "    if not batch:\n",
    "        return None  # Signal to skip this batch\n",
    "    \n",
    "    # Stack each field in the batch\n",
    "    collated_batch = {}\n",
    "    keys = batch[0].keys()\n",
    "    for key in keys:\n",
    "        collated_batch[key] = torch.stack([sample[key] for sample in batch])\n",
    "    \n",
    "    return collated_batch\n",
    "\n",
    "loader_test = DataLoader(dataset_test, batch_size=192, num_workers = 32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7be69dec-9b5a-43dc-8b90-79e12ed67a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config = {\n",
    "    'n_users' : dataset_test.max_user,\n",
    "    'n_items' : dataset_test.max_item,\n",
    "    'latent_d' : 64\n",
    "}\n",
    "\n",
    "model = Ex2Vec(config).to(device)\n",
    "checkpoint = torch.load('model_epoch_7.pt', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f77e3346-cfb7-4c50-a430-0c66a39d4520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ex2Vec(\n",
       "  (user_lamb): Embedding(13210, 1)\n",
       "  (user_bias): Embedding(13210, 1)\n",
       "  (item_bias): Embedding(3028, 1)\n",
       "  (embedding_user): Embedding(13210, 64)\n",
       "  (embedding_item): Embedding(3028, 64)\n",
       "  (logistic): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe889fbb-d7dd-45b6-a768-fd29177afd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 86765/86765 [30:23<00:00, 47.58it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_predictions = []\n",
    "\n",
    "pbar_test = tqdm(enumerate(loader_test), total=len(loader_test))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in pbar_test:\n",
    "        if batch is None:\n",
    "            pbar_test.update(1)\n",
    "            continue\n",
    "            \n",
    "        real = batch['real_values'].to(device)\n",
    "        user_id = batch['user_id'].to(device)\n",
    "        predict_items = batch['predict_items'].to(device)\n",
    "        history_items = batch['history_items'].to(device)\n",
    "        timedeltas = batch['timedeltas'].to(device)\n",
    "        weights = batch['weights'].to(device)\n",
    "\n",
    "        output = model(user_id, predict_items, history_items, timedeltas, weights)\n",
    "\n",
    "        scores = output.cpu().numpy()\n",
    "        item_indices = predict_items.cpu().numpy()\n",
    "\n",
    "        batch_size, item_count = scores.shape\n",
    "        max_item_index = item_indices.max()\n",
    "\n",
    "        result = np.full((batch_size, max_item_index + 1), np.nan)  # NaN for missing values\n",
    "\n",
    "        rows = np.arange(batch_size).reshape(-1, 1)\n",
    "        result[rows, item_indices] = scores  # Assign each score to the right item index column\n",
    "\n",
    "        df_batch = pd.DataFrame(result)\n",
    "        df_batch[0] = predict_items[:, -1].cpu().detach().numpy()\n",
    "\n",
    "        # # Optionally add user ID to keep track of rows\n",
    "        # df_batch[\"user_id\"] = user_id.cpu().numpy()\n",
    "        # df_batch.set_index(\"user_id\", inplace=True)\n",
    "\n",
    "        all_predictions.append(df_batch)\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            df_all = pd.concat(all_predictions, axis=0, ignore_index=True, copy=False)\n",
    "            df_all.to_parquet(f'./extended_results/extended_results_ordered_{i//10000}.parquet')\n",
    "            all_predictions = []\n",
    "\n",
    "        pbar_test.update(1)\n",
    "        \n",
    "df_all = pd.concat(all_predictions, axis=0, ignore_index=True, copy=False)\n",
    "df_all.to_parquet(f'./extended_results/extended_results_ordered_last.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d841dd7c-e58e-4050-a8df-b9da6685dc3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 667,  273,  396,  673,  673, 2431,  373])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_items[:, -1].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99908ea0-37a3-49ad-959e-200978ce0b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all = pd.concat(all_predictions, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de21530-950a-4a59-a316-b64b482a9994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85226dd1-6992-41cc-bdb1-0107ef7e0921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all.to_parquet('extended_results_ordered.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e6d4f6-4288-43f2-ab75-d7f748e5b72f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
